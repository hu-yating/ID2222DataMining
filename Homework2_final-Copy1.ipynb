{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "handled-spirituality",
   "metadata": {},
   "source": [
    "# Discovery of Frequent Itemsets and Association Rules"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "differential-school",
   "metadata": {},
   "source": [
    "The problem of discovering association rules between itemsets in a sales transaction database (a set of baskets) includes the following two sub-problems:\n",
    "\n",
    "1. Finding frequent itemsets with support at least s;\n",
    "2. Generating association rules with confidence at least c from the itemsets found in the first step.\n",
    "Remind that an association rule is an implication X → Y, where X and Y are itemsets such that X∩Y=∅. Support of the rule X → Y is the number of transactions that contain X⋃Y. Confidence of the rule X → Y the fraction of transactions containing X⋃Y in all transactions that contain X."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "efficient-carbon",
   "metadata": {},
   "source": [
    "You are to solve the first sub-problem: to implement the A-Priori algorithm for finding frequent itemsets with support at least s in a dataset of sales transactions. Remind that support of an itemset is the number of transactions containing the itemset. To test and evaluate your implementation, write a program that uses your A-Priori algorithm implementation to discover frequent itemsets with support at least s in a given dataset of sales transactions."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dynamic-windsor",
   "metadata": {},
   "source": [
    "The sale transaction dataset includes generated transactions (baskets) of hashed items (see Canvas)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "seven-toyota",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "100000"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "baskets = [i.strip().split() for i in open(\"T10I4D100K.dat\").readlines()]\n",
    "len(baskets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "liked-tension",
   "metadata": {},
   "outputs": [],
   "source": [
    "transactions = {} # Dictionary with transaction ID as key, and basket as value\n",
    "count = 0\n",
    "for basket in baskets:\n",
    "    count += 1\n",
    "    transactions[count] = basket\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "associate-attempt",
   "metadata": {},
   "outputs": [],
   "source": [
    "items = set() # Set of items from all baskets\n",
    "for i in transactions.values():\n",
    "    for j in i:\n",
    "        items.add(j) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "presidential-profile",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Count the frequency of each item\n",
    "def freq(k,items, transactions):\n",
    "    items_counts = dict() # Dictionary of item and its frequency\n",
    "    for i in items:\n",
    "        if k == 1:\n",
    "            temp_i = {i}\n",
    "        else:\n",
    "            temp_i = set(i)\n",
    "            \n",
    "        for j in transactions.items(): # and basket\n",
    "            if temp_i.issubset(set(j[1])): # if item is in basket\n",
    "                if i in items_counts:\n",
    "                    items_counts[i] += 1 # If already spotted/already in item-freq dict, add 1 to count\n",
    "                else:\n",
    "                    items_counts[i] = 1 # If not spotted yet, set count to 1\n",
    "    return items_counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "according-manhattan",
   "metadata": {},
   "outputs": [],
   "source": [
    "items_counts = freq(1, items, transactions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "dcb17b6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Support\n",
    "s_min = 5000\n",
    "L1 = [{j[0]:j[1] for j in freq(1,items, transactions).items() if j[1]>=s_min}]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a05306c8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Calculating candidates of size 2...\n"
     ]
    }
   ],
   "source": [
    "from itertools import combinations\n",
    "\n",
    "#candidates of len-k which are generated by combining itemsets from L_k-1 and L_1\n",
    "def C_k(k, prev_freq):\n",
    "    cand = []\n",
    "    print(f\"Calculating candidates of size {k}...\")\n",
    "    for i in prev_freq[0].keys():\n",
    "        if k-1 == 1:\n",
    "            temp = {i}\n",
    "            combs = combinations(list(temp.union(set(L1[0].keys()))), k) \n",
    "            cand = list(combs)\n",
    "\n",
    "        else:\n",
    "            temp = set(i)\n",
    "            for j in L1[0].keys():\n",
    "                if len(temp.union({j}))==k:\n",
    "                    cand.append(tuple(temp.union({j})))\n",
    "    return cand\n",
    "cand2 = C_k(2,L1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "5848f68c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Calculating frequent items of size 2\n"
     ]
    }
   ],
   "source": [
    "def L_k(k, candidates, threshold):\n",
    "    print(f\"Calculating frequent items of size {k}\")\n",
    "    Lk = [{j[0]:j[1] for j in freq(k,candidates, transactions).items() if j[1]>=threshold}]\n",
    "    return Lk\n",
    "L2 = L_k(2, cand2, 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "d5f87671",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Calculating candidates of size 3...\n"
     ]
    }
   ],
   "source": [
    "cand3 = C_k(3,L2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "43f74d6a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Calculating frequent items of size 3\n"
     ]
    }
   ],
   "source": [
    "L3 = L_k(3, cand3, 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "2bf8c600",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Calculating candidates of size 2...\n",
      "Calculating frequent items of size 2\n",
      "Calculating candidates of size 3...\n",
      "Calculating frequent items of size 3\n",
      "Calculating candidates of size 4...\n",
      "Calculating frequent items of size 4\n",
      "Calculating candidates of size 5...\n",
      "Calculating frequent items of size 5\n"
     ]
    }
   ],
   "source": [
    "# Look for frequent items until there is none\n",
    "\n",
    "result = [] # acts as the sets we need to look up \n",
    "lookup = [] # acts as lookup dictionary later on incl frozensets so order doesn't matter\n",
    "size = 1\n",
    "frequent_items = [] # excluding the frequency \n",
    "s_min = 10\n",
    "L1 = [{j[0]:j[1] for j in freq(1,items, transactions).items() if j[1]>=5000}]\n",
    "result.append(L1[0])\n",
    "lookup.append({frozenset([k]): v for k, v in L1[0].items()})\n",
    "#frequent_items.extend(list(L1[0].keys()))\n",
    "for x in list(L1[0].keys()):\n",
    "    frequent_items.append(tuple({x}))\n",
    "prev_freq = L1\n",
    "while True: \n",
    "    size+=1\n",
    "    candidates = C_k(size,prev_freq)\n",
    "    frequents = L_k(size,candidates,s_min)\n",
    "    prev_freq = frequents\n",
    "    if len(frequents[0])!=0:\n",
    "        frequent_items.extend(list(frequents[0].keys()))\n",
    "        result.append(frequents[0])\n",
    "        lookup.append({frozenset(k): v for k, v in prev_freq[0].items()})\n",
    "    else:\n",
    "        break\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "rough-packet",
   "metadata": {},
   "source": [
    "Optional task for extra bonus: Solve the second sub-problem, i.e., develop and implement an algorithm for generating association rules between frequent itemsets discovered by using the A-Priori algorithm in a dataset of sales transactions. The rules must have support at least s and confidence at least c, where s and c are given as input parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "cultural-garlic",
   "metadata": {},
   "outputs": [],
   "source": [
    "# For every subset A of frequent itemset I, rule is A -> I\\A\n",
    "# Since conf(ABC → D) ≥ conf(AB →CD) ≥ conf(A → BCD), first filter on association rules with large lhs\n",
    "\n",
    "# Suppose the largest frequent itemset found is of size k=4\n",
    "# Then, we start with all rules with 3 items on lhs and 1 item on rhs i.e. (3)-->(1) [size=4] \n",
    "# From (3)-->(1), we go to (2)-->(1) [size 3] we go to (1)-->(1) [size 2]\n",
    "\n",
    "# Then from (3)-->(1), we go to (2)-->(2) [size=4] and (1)-->(3) [size=4]\n",
    "# From (2)-->(2) we go to (1)-->(2) [size 3] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "prerequisite-commodity",
   "metadata": {},
   "outputs": [],
   "source": [
    "def association_rules(itemsets):\n",
    "    rules = []\n",
    "    for itemset in itemsets: # First generate rules for the largest itemsets\n",
    "        rule = {}\n",
    "        for i in range(len(itemset)):\n",
    "            rhs = itemset[i] # For 4-itemset, we check (3) --> (1)\n",
    "            lhs = set(itemset) - {rhs}\n",
    "            rule[tuple(lhs)] = rhs\n",
    "        rules.append(rule)\n",
    "    return rules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "pharmaceutical-observer",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_confidence(min_c,rules):\n",
    "    confidences = {}\n",
    "    for i in range(len(rules)):\n",
    "        rule = rules[i]\n",
    "        for lhs,rhs in zip(list(rule.keys()),list(rule.values())):\n",
    "            support = total_dict[frozenset(lhs)]\n",
    "            union_lhs_rhs = frozenset(tuple(set(lhs+tuple([rhs]))))\n",
    "            if union_lhs_rhs in total_dict:\n",
    "                support_union = total_dict[union_lhs_rhs]\n",
    "                confidence = support_union/support\n",
    "                confidences[(lhs,rhs)] = round(confidence,3)\n",
    "            #elif tuple(set(lhs+tuple([rhs]))) in total_dict:\n",
    "            #    confidence = support/total_dict[tuple(set(lhs+tuple([rhs])))]\n",
    "            #    confidences[str(lhs)+\"->\"+str(rhs)] = round(confidence,3)\n",
    "            else: \n",
    "                print(\"Not in dictionary\")\n",
    "    association_rules_at_least_c = {j[0]:j[1] for j in confidences.items() if j[1]>=min_c}\n",
    "    return association_rules_at_least_c\n",
    "    #return confidences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "described-occasion",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{(('829', '217', '419'), '529'): 0.909, (('217', '684', '368'), '829'): 1.091, (('217', '829', '684'), '368'): 1.0, (('684', '217', '368'), '829'): 1.091, (('684', '829', '217'), '368'): 1.0, (('829', '354', '419'), '529'): 1.0, (('829', '684', '419'), '368'): 0.857, (('684', '722', '368'), '829'): 0.875, (('684', '829', '722'), '368'): 1.105, (('722', '684', '368'), '829'): 0.875, (('722', '829', '684'), '368'): 1.105, (('217', '722', '494'), '368'): 1.071, (('722', '217', '494'), '368'): 1.071}\n"
     ]
    }
   ],
   "source": [
    "# Suppose the largest frequent itemset found is of size k=4\n",
    "# Then, we start with all rules with 3 items on lhs and 1 item on rhs i.e. (3)-->(1) [size=4] \n",
    "# From (3)-->(1), we go to (2)-->(1) [size 3] we go to (1)-->(1) [size 2]\n",
    "\n",
    "all_filtered_rules = []\n",
    "min_c = 0.8\n",
    "\n",
    "total_dict = {k: v for d in lookup for k, v in d.items()} # Create one dictionary as look-up\n",
    "# Start with the largest lhs\n",
    "largest_lhs = list(result[-1].keys())\n",
    "# Generate association rules for the largest lhs\n",
    "sub_rules = association_rules(largest_lhs) # For 4-itemset, we check (3)-->(1)\n",
    "# Calculate confidence and filter out\n",
    "filtered_rules = calculate_confidence(min_c,sub_rules)\n",
    "all_filtered_rules.append(filtered_rules)\n",
    "\n",
    "# Generate new rules based on the non-filtered-out rules\n",
    "sub = [rule[0] for rule in filtered_rules.keys()]\n",
    "while len(sub)>0 and len(sub[0])>1:\n",
    "    rules_sub = association_rules(sub) # For 4-itemset, we checked (3)-->(1), now (2)-->(1), and (1)-->(1)\n",
    "    filtered_rules = calculate_confidence(min_c,rules_sub)\n",
    "    all_filtered_rules.append(filtered_rules)\n",
    "    sub = [rule[0] for rule in filtered_rules.keys()]\n",
    "\n",
    "association_rules_filtered = {k: v for d in all_filtered_rules for k, v in d.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "embedded-concept",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{(('829', '217', '419'), '529'): 0.909,\n",
       " (('217', '684', '368'), '829'): 1.091,\n",
       " (('217', '829', '684'), '368'): 1.0,\n",
       " (('684', '217', '368'), '829'): 1.091,\n",
       " (('684', '829', '217'), '368'): 1.0,\n",
       " (('829', '354', '419'), '529'): 1.0,\n",
       " (('829', '684', '419'), '368'): 0.857,\n",
       " (('684', '722', '368'), '829'): 0.875,\n",
       " (('684', '829', '722'), '368'): 1.105,\n",
       " (('722', '684', '368'), '829'): 0.875,\n",
       " (('722', '829', '684'), '368'): 1.105,\n",
       " (('217', '722', '494'), '368'): 1.071,\n",
       " (('722', '217', '494'), '368'): 1.071}"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "association_rules_filtered"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "loving-vegetarian",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "19"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "total_dict[frozenset(('684', '829', '722'))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "removed-click",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "21"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "total_dict[frozenset(('684', '829', '722', '368'))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "electronic-institute",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{(('829', '217', '419'), '529'): 0.909, (('217', '684', '368'), '829'): 1.091, (('217', '829', '684'), '368'): 1.0, (('684', '217', '368'), '829'): 1.091, (('684', '829', '217'), '368'): 1.0, (('829', '354', '419'), '529'): 1.0, (('829', '684', '419'), '368'): 0.857, (('684', '722', '368'), '829'): 0.875, (('684', '829', '722'), '368'): 1.105, (('722', '684', '368'), '829'): 0.875, (('722', '829', '684'), '368'): 1.105, (('217', '722', '494'), '368'): 1.071, (('722', '217', '494'), '368'): 1.071}\n"
     ]
    }
   ],
   "source": [
    "# Then from (3)-->(1), we go to (2)-->(2) [size=4] and (1)-->(3) [size=4]\n",
    "# From (2)-->(2) we go to (1)-->(2) [size 3] \n",
    "\n",
    "all_filtered_rules = []\n",
    "min_c = 0.8\n",
    "\n",
    "total_dict = {k: v for d in lookup for k, v in d.items()} # Create one dictionary as look-up\n",
    "# Start with the largest lhs\n",
    "largest_lhs = list(result[-1].keys())\n",
    "# Generate association rules for the largest lhs\n",
    "sub_rules = association_rules(largest_lhs) # For 4-itemset, we check (3)-->(1)\n",
    "# Calculate confidence and filter out\n",
    "filtered_rules = calculate_confidence(min_c,sub_rules)\n",
    "print(filtered_rules)\n",
    "all_filtered_rules.append(filtered_rules)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "aware-sally",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{('419', '217'): ('829', '529'),\n",
       " ('829', '419'): ('684', '368'),\n",
       " ('829', '217'): ('684', '368'),\n",
       " ('684', '368'): ('829', '722'),\n",
       " ('217', '368'): ('829', '684'),\n",
       " ('217', '684'): ('829', '368'),\n",
       " ('829', '684'): ('722', '368'),\n",
       " ('684', '217'): ('829', '368'),\n",
       " ('419', '354'): ('829', '529'),\n",
       " ('829', '354'): ('419', '529'),\n",
       " ('419', '684'): ('829', '368'),\n",
       " ('722', '368'): ('829', '684'),\n",
       " ('684', '722'): ('829', '368'),\n",
       " ('829', '722'): ('684', '368'),\n",
       " ('722', '684'): ('829', '368'),\n",
       " ('722', '494'): ('217', '368'),\n",
       " ('217', '494'): ('722', '368'),\n",
       " ('217', '722'): ('494', '368'),\n",
       " ('722', '217'): ('494', '368')}"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rules_2_to_2 = {}\n",
    "for rule in filtered_rules.keys():\n",
    "    lhs = rule[0]\n",
    "    rhs = rule[1]\n",
    "    for i in range(len(lhs)):\n",
    "        new_lhs = set(lhs) - {lhs[i]}\n",
    "        new_rhs = set([rhs]).union({lhs[i]})\n",
    "        rules_2_to_2[tuple(new_lhs)] = tuple(new_rhs)\n",
    "rules_2_to_2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "certain-restaurant",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_confidence(min_c,rules):\n",
    "    confidences = {}\n",
    "    for i in range(len(rules)):\n",
    "        rule = rules[i]\n",
    "        for lhs,rhs in zip(list(rule.keys()),list(rule.values())):\n",
    "            support = total_dict[frozenset(lhs)]\n",
    "            union_lhs_rhs = frozenset(tuple(set(lhs+tuple([rhs]))))\n",
    "            if union_lhs_rhs in total_dict:\n",
    "                support_union = total_dict[union_lhs_rhs]\n",
    "                confidence = support_union/support\n",
    "                confidences[(lhs,rhs)] = round(confidence,3)\n",
    "            #elif tuple(set(lhs+tuple([rhs]))) in total_dict:\n",
    "            #    confidence = support/total_dict[tuple(set(lhs+tuple([rhs])))]\n",
    "            #    confidences[str(lhs)+\"->\"+str(rhs)] = round(confidence,3)\n",
    "            else: \n",
    "                print(\"Not in dictionary\")\n",
    "    association_rules_at_least_c = {j[0]:j[1] for j in confidences.items() if j[1]>=min_c}\n",
    "    return association_rules_at_least_c\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "ruled-interval",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{(('419', '217'), ('829', '529')): 0.029,\n",
       " (('829', '419'), ('684', '368')): 0.046,\n",
       " (('829', '217'), ('684', '368')): 0.044,\n",
       " (('684', '368'), ('829', '722')): 0.054,\n",
       " (('217', '368'), ('829', '684')): 0.04,\n",
       " (('217', '684'), ('829', '368')): 0.061,\n",
       " (('829', '684'), ('722', '368')): 0.06,\n",
       " (('684', '217'), ('829', '368')): 0.061,\n",
       " (('419', '354'), ('829', '529')): 0.038,\n",
       " (('829', '354'), ('419', '529')): 0.039,\n",
       " (('419', '684'), ('829', '368')): 0.077,\n",
       " (('722', '368'), ('829', '684')): 0.054,\n",
       " (('684', '722'), ('829', '368')): 0.047,\n",
       " (('829', '722'), ('684', '368')): 0.071,\n",
       " (('722', '684'), ('829', '368')): 0.047,\n",
       " (('722', '494'), ('217', '368')): 0.066,\n",
       " (('217', '494'), ('722', '368')): 0.082,\n",
       " (('217', '722'), ('494', '368')): 0.03,\n",
       " (('722', '217'), ('494', '368')): 0.03}"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "min_c = 0\n",
    "confidences_2_to_2 = {}\n",
    "for lhs in rules_2_to_2:\n",
    "    support = total_dict[frozenset(lhs)]\n",
    "    rhs = rules_2_to_2[lhs]\n",
    "    union_lhs_rhs = frozenset(tuple(lhs+rhs))\n",
    "    if union_lhs_rhs in total_dict:\n",
    "        support_union = total_dict[union_lhs_rhs]\n",
    "        confidence = support_union/support\n",
    "        confidences_2_to_2[(lhs,rhs)] = round(confidence,3)\n",
    "    else: \n",
    "        print(\"Not in dictionary\")\n",
    "association_rules_at_least_c = {j[0]:j[1] for j in confidences_2_to_2.items() if j[1]>=min_c}\n",
    "association_rules_at_least_c"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "extraordinary-edwards",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
